{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e16f6850-2dd7-43c4-a600-6a0f6f8f5f15",
   "metadata": {},
   "source": [
    "### Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "779bc156-04d9-449d-8f19-c2856c4def8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TOP\\anaconda3\\envs\\evn8\\lib\\site-packages\\scipy\\__init__.py:177: UserWarning: A NumPy version >=1.18.5 and <1.26.0 is required for this version of SciPy (detected version 1.18.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_federated as tff\n",
    "from collections import Counter\n",
    "from scipy.stats import  ks_2samp, chi2_contingency\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# تنظیم بذر تصادفی برای تکرارپذیری\n",
    "# np.random.seed(42)\n",
    "# tf.random.set_seed(42)\n",
    "\n",
    "# تنظیم زمینه اجرایی محلی\n",
    "tff.backends.native.set_local_execution_context()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d246e04-aec6-4a91-8693-ef6fa34d0854",
   "metadata": {},
   "source": [
    "### Create Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6987f8a1-152d-4002-a8ee-cd772b6324a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# تابع ساخت مدل MLP\n",
    "def create_mnist_mlp_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, activation='relu', input_shape=(784,)))  # ورودی به شکل مسطح شده (28x28 = 784)\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(10, activation='softmax'))  # لایه خروجی با 10 کلاس (softmax)\n",
    "    optimizer = Adam(learning_rate=0.01)\n",
    "    # model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])  # کامپایل کردن مدل\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4062329-af4d-4e7f-ac24-3d06b83fa354",
   "metadata": {},
   "source": [
    "### Load and Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4565aca-8960-4e75-be06-fee6f7efd05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# لود کردن داده‌های MNIST\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# نرمال‌سازی داده‌ها\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "# مسطح کردن تصاویر\n",
    "x_train = x_train.reshape(-1, 784)\n",
    "x_test = x_test.reshape(-1, 784)\n",
    "\n",
    "# کدگذاری لیبل‌ها\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "# تقسیم داده‌ها بین 10 کلاینت\n",
    "num_clients = 30\n",
    "client_data = np.array_split(x_train, num_clients)\n",
    "client_labels = np.array_split(y_train, num_clients)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed859481-34a4-42c2-8527-a7f0fc08f674",
   "metadata": {},
   "source": [
    "### Bug Injection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8711f8c-428b-4b53-83ac-58ad1d57a7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# لیستی از کلاینت‌هایی که باید دوبار نرمال‌سازی شوند\n",
    "clients_to_double_normalize = [0,15,19]  # کلاینت‌های 3,6 (با اندیس صفر شروع می‌شود)\n",
    "\n",
    "# دوبار نرمال‌سازی کلاینت‌های مشخص‌شده\n",
    "for client_index in clients_to_double_normalize:\n",
    "    client_data[client_index] = client_data[client_index] / 255.0\n",
    "\n",
    "# پوشه‌ای برای ذخیره پارامترهای کلاینت‌ها ایجاد می‌کنیم\n",
    "if not os.path.exists('client_params'):\n",
    "    os.makedirs('client_params')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2523be-3749-4df3-bb41-b3470a3b5cac",
   "metadata": {},
   "source": [
    "### Models training & save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "459da808-8791-4fa9-8768-2b7b2257b134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for client 1\n",
      "Epoch 1/5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.2745 - accuracy: 0.2150\n",
      "Epoch 2/5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 1.8639 - accuracy: 0.4620\n",
      "Epoch 3/5\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.2572 - accuracy: 0.5810\n",
      "Epoch 4/5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.9718 - accuracy: 0.6795\n",
      "Epoch 5/5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.8391 - accuracy: 0.7285\n",
      "Training model for client 2\n",
      "Epoch 1/5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 1.0026 - accuracy: 0.6870\n",
      "Epoch 2/5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.3553 - accuracy: 0.8895\n",
      "Epoch 3/5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.2412 - accuracy: 0.9295\n",
      "Epoch 4/5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1583 - accuracy: 0.9480\n",
      "Epoch 5/5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1247 - accuracy: 0.9580\n",
      "Training model for client 3\n",
      "Epoch 1/5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.0299 - accuracy: 0.6755\n",
      "Epoch 2/5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.3853 - accuracy: 0.8815\n",
      "Epoch 3/5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2547 - accuracy: 0.9190\n",
      "Epoch 4/5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1990 - accuracy: 0.9355\n",
      "Epoch 5/5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1333 - accuracy: 0.9590\n",
      "Training model for client 4\n",
      "Epoch 1/5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.0711 - accuracy: 0.6580\n",
      "Epoch 2/5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.4096 - accuracy: 0.8790\n",
      "Epoch 3/5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2741 - accuracy: 0.9175\n",
      "Epoch 4/5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1984 - accuracy: 0.9400\n",
      "Epoch 5/5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1438 - accuracy: 0.9610\n",
      "Training model for client 5\n",
      "Epoch 1/5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.0776 - accuracy: 0.6600\n",
      "Epoch 2/5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.4488 - accuracy: 0.8650\n",
      "Epoch 3/5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.3068 - accuracy: 0.9060\n",
      "Epoch 4/5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.2241 - accuracy: 0.9310\n",
      "Epoch 5/5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1699 - accuracy: 0.9465\n",
      "Training model for client 6\n",
      "Epoch 1/5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 1.0222 - accuracy: 0.6810\n",
      "Epoch 2/5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.3849 - accuracy: 0.8760\n",
      "Epoch 3/5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.2580 - accuracy: 0.9130\n",
      "Epoch 4/5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1933 - accuracy: 0.9420\n",
      "Epoch 5/5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1246 - accuracy: 0.9615\n",
      "Training model for client 7\n",
      "Epoch 1/5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 1.0957 - accuracy: 0.6515\n",
      "Epoch 2/5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.4511 - accuracy: 0.8560\n",
      "Epoch 3/5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2911 - accuracy: 0.9125\n",
      "Epoch 4/5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2229 - accuracy: 0.9250\n",
      "Epoch 5/5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1432 - accuracy: 0.9565\n",
      "Training model for client 8\n",
      "Epoch 1/5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.1090 - accuracy: 0.6410\n",
      "Epoch 2/5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.4200 - accuracy: 0.8680\n",
      "Epoch 3/5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.2965 - accuracy: 0.9110\n",
      "Epoch 4/5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.2139 - accuracy: 0.9305\n",
      "Epoch 5/5\n",
      "63/63 [==============================] - 1s 8ms/step - loss: 0.1620 - accuracy: 0.9460\n",
      "Training model for client 9\n",
      "Epoch 1/5\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.1347 - accuracy: 0.6495\n",
      "Epoch 2/5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.4552 - accuracy: 0.8620\n",
      "Epoch 3/5\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.2790 - accuracy: 0.9100\n",
      "Epoch 4/5\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.1919 - accuracy: 0.9420\n",
      "Epoch 5/5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1211 - accuracy: 0.9625\n",
      "Training model for client 10\n",
      "Epoch 1/5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.0225 - accuracy: 0.6790\n",
      "Epoch 2/5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.3639 - accuracy: 0.8875\n",
      "Epoch 3/5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.2092 - accuracy: 0.9415\n",
      "Epoch 4/5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1949 - accuracy: 0.9400\n",
      "Epoch 5/5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1275 - accuracy: 0.9655\n",
      "Training model for client 11\n",
      "Epoch 1/5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.0137 - accuracy: 0.6715\n",
      "Epoch 2/5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.3887 - accuracy: 0.8875\n",
      "Epoch 3/5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2920 - accuracy: 0.9050\n",
      "Epoch 4/5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1901 - accuracy: 0.9435\n",
      "Epoch 5/5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1579 - accuracy: 0.9490\n",
      "Training model for client 12\n",
      "Epoch 1/5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.0744 - accuracy: 0.6560\n",
      "Epoch 2/5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.4138 - accuracy: 0.8720\n",
      "Epoch 3/5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2789 - accuracy: 0.9130\n",
      "Epoch 4/5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1964 - accuracy: 0.9360\n",
      "Epoch 5/5\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.1399 - accuracy: 0.9540\n",
      "Training model for client 13\n",
      "Epoch 1/5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 1.0280 - accuracy: 0.6760\n",
      "Epoch 2/5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.3819 - accuracy: 0.8735\n",
      "Epoch 3/5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.2556 - accuracy: 0.9215\n",
      "Epoch 4/5\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.1964 - accuracy: 0.9410\n",
      "Epoch 5/5\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 0.1240 - accuracy: 0.9580\n",
      "Training model for client 14\n",
      "Epoch 1/5\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.0298 - accuracy: 0.6610\n",
      "Epoch 2/5\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.3797 - accuracy: 0.8810\n",
      "Epoch 3/5\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.2922 - accuracy: 0.9060\n",
      "Epoch 4/5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1900 - accuracy: 0.9400\n",
      "Epoch 5/5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1464 - accuracy: 0.9560\n",
      "Training model for client 15\n",
      "Epoch 1/5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.0616 - accuracy: 0.6545\n",
      "Epoch 2/5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.4540 - accuracy: 0.8665\n",
      "Epoch 3/5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2847 - accuracy: 0.9105\n",
      "Epoch 4/5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2059 - accuracy: 0.9360\n",
      "Epoch 5/5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1480 - accuracy: 0.9535\n",
      "Training model for client 16\n",
      "Epoch 1/5\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 2.2739 - accuracy: 0.2300\n",
      "Epoch 2/5\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.8885 - accuracy: 0.3785\n",
      "Epoch 3/5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 1.3403 - accuracy: 0.5320\n",
      "Epoch 4/5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 1.0864 - accuracy: 0.6165\n",
      "Epoch 5/5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.9210 - accuracy: 0.6990\n",
      "Training model for client 17\n",
      "Epoch 1/5\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.0596 - accuracy: 0.6675\n",
      "Epoch 2/5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.3903 - accuracy: 0.8830\n",
      "Epoch 3/5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.2790 - accuracy: 0.9155\n",
      "Epoch 4/5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1764 - accuracy: 0.9440\n",
      "Epoch 5/5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1265 - accuracy: 0.9625\n",
      "Training model for client 18\n",
      "Epoch 1/5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.0623 - accuracy: 0.6465\n",
      "Epoch 2/5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.3970 - accuracy: 0.8790\n",
      "Epoch 3/5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2613 - accuracy: 0.9165\n",
      "Epoch 4/5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1881 - accuracy: 0.9400\n",
      "Epoch 5/5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1511 - accuracy: 0.9460\n",
      "Training model for client 19\n",
      "Epoch 1/5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.0803 - accuracy: 0.6580\n",
      "Epoch 2/5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.4217 - accuracy: 0.8660\n",
      "Epoch 3/5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.3027 - accuracy: 0.9080\n",
      "Epoch 4/5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.2162 - accuracy: 0.9280\n",
      "Epoch 5/5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1445 - accuracy: 0.9580\n",
      "Training model for client 20\n",
      "Epoch 1/5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.2762 - accuracy: 0.1995\n",
      "Epoch 2/5\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.8805 - accuracy: 0.3810\n",
      "Epoch 3/5\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.3069 - accuracy: 0.5660\n",
      "Epoch 4/5\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.9844 - accuracy: 0.6580\n",
      "Epoch 5/5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.8441 - accuracy: 0.7190\n",
      "Training model for client 21\n",
      "Epoch 1/5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 1.0724 - accuracy: 0.6540\n",
      "Epoch 2/5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.4230 - accuracy: 0.8675\n",
      "Epoch 3/5\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.2740 - accuracy: 0.9115\n",
      "Epoch 4/5\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.1856 - accuracy: 0.9430\n",
      "Epoch 5/5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1298 - accuracy: 0.9615\n",
      "Training model for client 22\n",
      "Epoch 1/5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 1.0800 - accuracy: 0.6570\n",
      "Epoch 2/5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.4236 - accuracy: 0.8705\n",
      "Epoch 3/5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2839 - accuracy: 0.9040\n",
      "Epoch 4/5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1884 - accuracy: 0.9410\n",
      "Epoch 5/5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1262 - accuracy: 0.9635\n",
      "Training model for client 23\n",
      "Epoch 1/5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 1.0703 - accuracy: 0.6475\n",
      "Epoch 2/5\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.4179 - accuracy: 0.8610\n",
      "Epoch 3/5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.2965 - accuracy: 0.9015\n",
      "Epoch 4/5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.2080 - accuracy: 0.9340\n",
      "Epoch 5/5\n",
      "63/63 [==============================] - 1s 8ms/step - loss: 0.1518 - accuracy: 0.9520\n",
      "Training model for client 24\n",
      "Epoch 1/5\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.0698 - accuracy: 0.6495\n",
      "Epoch 2/5\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.4365 - accuracy: 0.8450\n",
      "Epoch 3/5\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.2973 - accuracy: 0.9110\n",
      "Epoch 4/5\n",
      "63/63 [==============================] - 1s 8ms/step - loss: 0.2073 - accuracy: 0.9375\n",
      "Epoch 5/5\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.1474 - accuracy: 0.9530\n",
      "Training model for client 25\n",
      "Epoch 1/5\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.0840 - accuracy: 0.6425\n",
      "Epoch 2/5\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.4402 - accuracy: 0.8595\n",
      "Epoch 3/5\n",
      "63/63 [==============================] - 1s 8ms/step - loss: 0.2747 - accuracy: 0.9080\n",
      "Epoch 4/5\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 0.1992 - accuracy: 0.9380\n",
      "Epoch 5/5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1504 - accuracy: 0.9500\n",
      "Training model for client 26\n",
      "Epoch 1/5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.0233 - accuracy: 0.6750\n",
      "Epoch 2/5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.4008 - accuracy: 0.8810\n",
      "Epoch 3/5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2676 - accuracy: 0.9250\n",
      "Epoch 4/5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1891 - accuracy: 0.9415\n",
      "Epoch 5/5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1430 - accuracy: 0.9555\n",
      "Training model for client 27\n",
      "Epoch 1/5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.1132 - accuracy: 0.6430\n",
      "Epoch 2/5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.4484 - accuracy: 0.8640\n",
      "Epoch 3/5\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.2821 - accuracy: 0.9120\n",
      "Epoch 4/5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.2094 - accuracy: 0.9385\n",
      "Epoch 5/5\n",
      "63/63 [==============================] - 1s 8ms/step - loss: 0.1564 - accuracy: 0.9520\n",
      "Training model for client 28\n",
      "Epoch 1/5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.9977 - accuracy: 0.6865\n",
      "Epoch 2/5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.3551 - accuracy: 0.8845\n",
      "Epoch 3/5\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.2540 - accuracy: 0.9230\n",
      "Epoch 4/5\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.1694 - accuracy: 0.9460\n",
      "Epoch 5/5\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.1266 - accuracy: 0.9570\n",
      "Training model for client 29\n",
      "Epoch 1/5\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.0267 - accuracy: 0.6750\n",
      "Epoch 2/5\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.3501 - accuracy: 0.8955\n",
      "Epoch 3/5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2361 - accuracy: 0.9240\n",
      "Epoch 4/5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1528 - accuracy: 0.9565\n",
      "Epoch 5/5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1278 - accuracy: 0.9575\n",
      "Training model for client 30\n",
      "Epoch 1/5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.8669 - accuracy: 0.7315\n",
      "Epoch 2/5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2715 - accuracy: 0.9225\n",
      "Epoch 3/5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1550 - accuracy: 0.9585\n",
      "Epoch 4/5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0984 - accuracy: 0.9715\n",
      "Epoch 5/5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0716 - accuracy: 0.9780\n",
      "Training completed for all clients.\n"
     ]
    }
   ],
   "source": [
    "# آموزش مدل‌ها برای هر کلاینت و ذخیره پارامترها\n",
    "client_models = [] #لیست برای ذخیره مدل‌های هر کلاینت\n",
    "\n",
    "base_model = create_mnist_mlp_model()  # مدل پایه\n",
    "\n",
    "#base_model.save_weights('base_model_weights.h5') # ذخیره وزن‌های مدل پایه\n",
    "\n",
    "for i in range(num_clients):\n",
    "    print(f\"Training model for client {i+1}\")    \n",
    "    model=create_mnist_mlp_model()\n",
    "    model.set_weights(base_model.get_weights())\n",
    "    \n",
    "    # بررسی یکسان بودن وزن‌ها قبل از آموزش\n",
    "    # m1 = model.get_weights()\n",
    "    # b1 = base_model.get_weights()\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])  # کامپایل کردن مدل\n",
    "    # for index, (base_w, clone_w) in enumerate(zip(m1, b1)):\n",
    "    #     if not np.array_equal(base_w, clone_w):\n",
    "    #         print(f\"Weights at index {index} are different.\")\n",
    "    #     else:\n",
    "    #         print(f\"Weights at index {index} are identical.\")\n",
    "    model.fit(client_data[i], client_labels[i], epochs=5, batch_size=32, verbose=1)\n",
    "    #model.save_weights(f'client_params/client_{i+1}_weights.h5') # ذخیره وزن‌های هر کلاینت\n",
    "    #print(\"**********\",model.predict(x_test[:100].argmax(axis=1)))\n",
    "    client_models.append(model)  # افزودن مدل به لیست\n",
    "print(\"Training completed for all clients.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736c8110-a535-4905-88f2-53db5aef46fc",
   "metadata": {},
   "source": [
    "### Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b64986f1-6a6d-4ce6-b081-853d680d50e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference Matrix for Delta Class:\n",
      "[[   0 3056 2978 2821 2862 2966 3028 2894 2956 3121 3067 3051 2961 3093\n",
      "  3032 1792 2882 2952 3166 1131 3002 2993 3033 3041 3054 2886 2937 2919\n",
      "  2992 2877]\n",
      " [3056    0  820  838  809  848  794  830  814  909  795  764  737 1007\n",
      "   793 3390  786  864  980 3136  915  955  831  838  789  816  851  899\n",
      "   718 1038]\n",
      " [2978  820    0  747  767  808  743  795  748  832  881  764  745  992\n",
      "   790 3365  687  768  856 3071  905  916  864  817  715  791  766  867\n",
      "   752  903]\n",
      " [2821  838  747    0  797  837  851  786  798  852  885  823  720 1006\n",
      "   841 3195  748  770 1009 2887  895  884  822  806  807  751  765  836\n",
      "   744  960]\n",
      " [2862  809  767  797    0  843  827  822  761  943  886  843  682 1058\n",
      "   855 3224  752  856 1032 2911  988  933  835  832  788  831  752  876\n",
      "   767  953]\n",
      " [2966  848  808  837  843    0  802  839  874  984  902  838  841  949\n",
      "   767 3385  869  827 1006 3067  866  929  907  832  784  876  912  878\n",
      "   781 1027]\n",
      " [3028  794  743  851  827  802    0  805  781  898  792  753  830 1044\n",
      "   752 3384  729  821  879 3120  900  932  844  752  725  842  782  896\n",
      "   778  997]\n",
      " [2894  830  795  786  822  839  805    0  782  971  797  823  793  974\n",
      "   786 3279  736  785  961 2995  932  930  753  747  739  794  803  820\n",
      "   736  970]\n",
      " [2956  814  748  798  761  874  781  782    0  966  834  831  777 1015\n",
      "   784 3376  713  829  913 3052  984  974  870  800  748  820  776  837\n",
      "   754  923]\n",
      " [3121  909  832  852  943  984  898  971  966    0 1023  873  880 1197\n",
      "   856 3430  914  865  938 3183 1092 1071  970  983  899  930  948 1081\n",
      "   816 1090]\n",
      " [3067  795  881  885  886  902  792  797  834 1023    0  860  829  988\n",
      "   899 3404  825  960  963 3104 1034  967  866  787  786  842  853  880\n",
      "   784 1033]\n",
      " [3051  764  764  823  843  838  753  823  831  873  860    0  835 1073\n",
      "   709 3367  794  794  874 3141  902  959  853  757  730  898  839  946\n",
      "   758 1029]\n",
      " [2961  737  745  720  682  841  830  793  777  880  829  835    0  980\n",
      "   832 3302  741  785  963 3001  991  855  756  818  792  786  753  816\n",
      "   671 1004]\n",
      " [3093 1007  992 1006 1058  949 1044  974 1015 1197  988 1073  980    0\n",
      "   991 3463  949 1057 1178 3190 1108 1072 1072  998  939  934 1082  955\n",
      "  1005 1172]\n",
      " [3032  793  790  841  855  767  752  786  784  856  899  709  832  991\n",
      "     0 3372  794  736  855 3178  872  962  850  773  718  829  863  909\n",
      "   754 1012]\n",
      " [1792 3390 3365 3195 3224 3385 3384 3279 3376 3430 3404 3367 3302 3463\n",
      "  3372    0 3308 3351 3574 1643 3359 3275 3333 3365 3410 3237 3305 3305\n",
      "  3328 3259]\n",
      " [2882  786  687  748  752  869  729  736  713  914  825  794  741  949\n",
      "   794 3308    0  767  851 3008  914  903  829  790  675  779  779  810\n",
      "   741  982]\n",
      " [2952  864  768  770  856  827  821  785  829  865  960  794  785 1057\n",
      "   736 3351  767    0  943 3061  855  925  778  784  772  851  841  919\n",
      "   721 1028]\n",
      " [3166  980  856 1009 1032 1006  879  961  913  938  963  874  963 1178\n",
      "   855 3574  851  943    0 3264 1113 1189 1035  967  860  976 1038 1108\n",
      "   890 1159]\n",
      " [1131 3136 3071 2887 2911 3067 3120 2995 3052 3183 3104 3141 3001 3190\n",
      "  3178 1643 3008 3061 3264    0 3134 3023 3058 3129 3135 2984 3005 2967\n",
      "  3037 2959]\n",
      " [3002  915  905  895  988  866  900  932  984 1092 1034  902  991 1108\n",
      "   872 3359  914  855 1113 3134    0 1031  997  927  895  987  943  946\n",
      "   946 1162]\n",
      " [2993  955  916  884  933  929  932  930  974 1071  967  959  855 1072\n",
      "   962 3275  903  925 1189 3023 1031    0  885  968  919  918  918  827\n",
      "   881 1137]\n",
      " [3033  831  864  822  835  907  844  753  870  970  866  853  756 1072\n",
      "   850 3333  829  778 1035 3058  997  885    0  771  782  826  806  794\n",
      "   735 1045]\n",
      " [3041  838  817  806  832  832  752  747  800  983  787  757  818  998\n",
      "   773 3365  790  784  967 3129  927  968  771    0  721  873  839  827\n",
      "   750  996]\n",
      " [3054  789  715  807  788  784  725  739  748  899  786  730  792  939\n",
      "   718 3410  675  772  860 3135  895  919  782  721    0  825  787  827\n",
      "   716  987]\n",
      " [2886  816  791  751  831  876  842  794  820  930  842  898  786  934\n",
      "   829 3237  779  851  976 2984  987  918  826  873  825    0  832  893\n",
      "   781 1001]\n",
      " [2937  851  766  765  752  912  782  803  776  948  853  839  753 1082\n",
      "   863 3305  779  841 1038 3005  943  918  806  839  787  832    0  839\n",
      "   804 1010]\n",
      " [2919  899  867  836  876  878  896  820  837 1081  880  946  816  955\n",
      "   909 3305  810  919 1108 2967  946  827  794  827  827  893  839    0\n",
      "   843 1046]\n",
      " [2992  718  752  744  767  781  778  736  754  816  784  758  671 1005\n",
      "   754 3328  741  721  890 3037  946  881  735  750  716  781  804  843\n",
      "     0  971]\n",
      " [2877 1038  903  960  953 1027  997  970  923 1090 1033 1029 1004 1172\n",
      "  1012 3259  982 1028 1159 2959 1162 1137 1045  996  987 1001 1010 1046\n",
      "   971    0]]\n"
     ]
    }
   ],
   "source": [
    "def delta_class(models, x_test):\n",
    "    num_models = len(models)\n",
    "    argmax_preds = [model.predict(x_test).argmax(axis=1) for model in models]   # پیش‌بینی‌ها برای هر مدل\n",
    "    diffs_matrix = np.zeros((num_models, num_models), dtype=int)    # ماتریس مربعی برای ذخیره تفاوت‌ها\n",
    "    for i in range(num_models):       # پر کردن ماتریس با تفاوت پیش‌بینی‌ها\n",
    "        for j in range(i+1, num_models):\n",
    "            diffs = np.sum(argmax_preds[i] != argmax_preds[j])  # تعداد تفاوت‌های پیش‌بینی بین مدل i و مدل j\n",
    "            diffs_matrix[i, j] = diffs\n",
    "            diffs_matrix[j, i] = diffs  # ماتریس متقارن است\n",
    "    return diffs_matrix\n",
    "\n",
    "# اندازه‌گیری زمان\n",
    "start_time = time.time()\n",
    "diff_class = delta_class(client_models, x_test)\n",
    "print(\"Difference Matrix for Delta Class:\")\n",
    "print(diff_class)\n",
    "\n",
    "# end_time = time.time()\n",
    "# execution_time = end_time - start_time\n",
    "# print(f\"\\nExecution time for delta_class: {execution_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0868a773-58cd-477e-a7e4-714f6501c957",
   "metadata": {},
   "source": [
    "### Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86b65fb0-aa33-4176-b505-2ea19e19c353",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance Matrix for Delta Score:\n",
      "[[   0. 3732. 3795. 3600. 3659. 3802. 3886. 3744. 3657. 3883. 3923. 3782.\n",
      "  3712. 3989. 3779. 1890. 3619. 3759. 4009. 1232. 3927. 3908. 3902. 3881.\n",
      "  3816. 3751. 3702. 3633. 3692. 3665.]\n",
      " [3732.    0. 2140. 2249. 2168. 2141. 2148. 2314. 2114. 2222. 2056. 2052.\n",
      "  2052. 2473. 2083. 4056. 2028. 2209. 2421. 3819. 2369. 2415. 2215. 2251.\n",
      "  2091. 2201. 2220. 2173. 1866. 2408.]\n",
      " [3795. 2140.    0. 2187. 2177. 2149. 2175. 2315. 2117. 2300. 2285. 2195.\n",
      "  2122. 2497. 2087. 4166. 1953. 2193. 2288. 3926. 2418. 2478. 2399. 2323.\n",
      "  2117. 2253. 2170. 2255. 2027. 2289.]\n",
      " [3600. 2249. 2187.    0. 2194. 2298. 2347. 2239. 2217. 2253. 2318. 2239.\n",
      "  2087. 2502. 2270. 3902. 2138. 2142. 2664. 3654. 2467. 2358. 2196. 2265.\n",
      "  2267. 2155. 2138. 2224. 2075. 2415.]\n",
      " [3659. 2168. 2177. 2194.    0. 2229. 2303. 2290. 2178. 2410. 2287. 2248.\n",
      "  2061. 2580. 2294. 3977. 2059. 2261. 2649. 3715. 2517. 2447. 2242. 2265.\n",
      "  2176. 2221. 2055. 2229. 2061. 2329.]\n",
      " [3802. 2141. 2149. 2298. 2229.    0. 2186. 2317. 2286. 2416. 2290. 2154.\n",
      "  2203. 2421. 2097. 4141. 2186. 2191. 2450. 3899. 2290. 2475. 2396. 2220.\n",
      "  2101. 2305. 2310. 2266. 2038. 2395.]\n",
      " [3886. 2148. 2175. 2347. 2303. 2186.    0. 2321. 2158. 2473. 2154. 2175.\n",
      "  2289. 2521. 2160. 4206. 2009. 2259. 2457. 3963. 2394. 2458. 2337. 2172.\n",
      "  2073. 2361. 2272. 2301. 2077. 2427.]\n",
      " [3744. 2314. 2315. 2239. 2290. 2317. 2321.    0. 2189. 2541. 2220. 2313.\n",
      "  2323. 2507. 2232. 4100. 2151. 2228. 2586. 3827. 2481. 2529. 2216. 2153.\n",
      "  2189. 2217. 2224. 2271. 2136. 2446.]\n",
      " [3657. 2114. 2117. 2217. 2178. 2286. 2158. 2189.    0. 2350. 2165. 2176.\n",
      "  2120. 2507. 2124. 4040. 1950. 2206. 2337. 3770. 2448. 2466. 2314. 2231.\n",
      "  2066. 2187. 2107. 2139. 2020. 2214.]\n",
      " [3883. 2222. 2300. 2253. 2410. 2416. 2473. 2541. 2350.    0. 2495. 2193.\n",
      "  2272. 2755. 2223. 4122. 2286. 2259. 2485. 3955. 2669. 2620. 2483. 2528.\n",
      "  2333. 2330. 2369. 2459. 2080. 2560.]\n",
      " [3923. 2056. 2285. 2318. 2287. 2290. 2154. 2220. 2165. 2495.    0. 2191.\n",
      "  2181. 2419. 2307. 4208. 2161. 2386. 2531. 3963. 2553. 2432. 2334. 2199.\n",
      "  2134. 2196. 2257. 2229. 2041. 2445.]\n",
      " [3782. 2052. 2195. 2239. 2248. 2154. 2175. 2313. 2176. 2193. 2191.    0.\n",
      "  2222. 2535. 2061. 4055. 2120. 2126. 2405. 3881. 2414. 2446. 2258. 2166.\n",
      "  2026. 2275. 2155. 2259. 1973. 2421.]\n",
      " [3712. 2052. 2122. 2087. 2061. 2203. 2289. 2323. 2120. 2272. 2181. 2222.\n",
      "     0. 2475. 2233. 4005. 2042. 2199. 2524. 3747. 2505. 2336. 2223. 2280.\n",
      "  2183. 2127. 2136. 2126. 1927. 2363.]\n",
      " [3989. 2473. 2497. 2502. 2580. 2421. 2521. 2507. 2507. 2755. 2419. 2535.\n",
      "  2475.    0. 2447. 4301. 2327. 2555. 2791. 4052. 2651. 2601. 2613. 2434.\n",
      "  2368. 2392. 2552. 2369. 2405. 2678.]\n",
      " [3779. 2083. 2087. 2270. 2294. 2097. 2160. 2232. 2124. 2223. 2307. 2061.\n",
      "  2233. 2447.    0. 4115. 2099. 2025. 2242. 3954. 2326. 2505. 2321. 2158.\n",
      "  2022. 2238. 2266. 2296. 1975. 2385.]\n",
      " [1890. 4056. 4166. 3902. 3977. 4141. 4206. 4100. 4040. 4122. 4208. 4055.\n",
      "  4005. 4301. 4115.    0. 3995. 4082. 4404. 1760. 4232. 4100. 4130. 4151.\n",
      "  4150. 4014. 4023. 3969. 3969. 4012.]\n",
      " [3619. 2028. 1953. 2138. 2059. 2186. 2009. 2151. 1950. 2286. 2161. 2120.\n",
      "  2042. 2327. 2099. 3995.    0. 2051. 2264. 3743. 2373. 2350. 2288. 2175.\n",
      "  1934. 2107. 2127. 2089. 1911. 2291.]\n",
      " [3759. 2209. 2193. 2142. 2261. 2191. 2259. 2228. 2206. 2259. 2386. 2126.\n",
      "  2199. 2555. 2025. 4082. 2051.    0. 2467. 3880. 2242. 2391. 2172. 2184.\n",
      "  2172. 2320. 2212. 2272. 1963. 2414.]\n",
      " [4009. 2421. 2288. 2664. 2649. 2450. 2457. 2586. 2337. 2485. 2531. 2405.\n",
      "  2524. 2791. 2242. 4404. 2264. 2467.    0. 4175. 2762. 2918. 2700. 2596.\n",
      "  2330. 2575. 2639. 2651. 2279. 2635.]\n",
      " [1232. 3819. 3926. 3654. 3715. 3899. 3963. 3827. 3770. 3955. 3963. 3881.\n",
      "  3747. 4052. 3954. 1760. 3743. 3880. 4175.    0. 4061. 3881. 3910. 3933.\n",
      "  3916. 3821. 3762. 3668. 3742. 3733.]\n",
      " [3927. 2369. 2418. 2467. 2517. 2290. 2394. 2481. 2448. 2669. 2553. 2414.\n",
      "  2505. 2651. 2326. 4232. 2373. 2242. 2762. 4061.    0. 2683. 2597. 2427.\n",
      "  2394. 2484. 2433. 2414. 2340. 2705.]\n",
      " [3908. 2415. 2478. 2358. 2447. 2475. 2458. 2529. 2466. 2620. 2432. 2446.\n",
      "  2336. 2601. 2505. 4100. 2350. 2391. 2918. 3881. 2683.    0. 2317. 2404.\n",
      "  2451. 2404. 2411. 2144. 2265. 2689.]\n",
      " [3902. 2215. 2399. 2196. 2242. 2396. 2337. 2216. 2314. 2483. 2334. 2258.\n",
      "  2223. 2613. 2321. 4130. 2288. 2172. 2700. 3910. 2597. 2317.    0. 2171.\n",
      "  2215. 2303. 2266. 2152. 2086. 2581.]\n",
      " [3881. 2251. 2323. 2265. 2265. 2220. 2172. 2153. 2231. 2528. 2199. 2166.\n",
      "  2280. 2434. 2158. 4151. 2175. 2184. 2596. 3933. 2427. 2404. 2171.    0.\n",
      "  2088. 2302. 2248. 2183. 2040. 2491.]\n",
      " [3816. 2091. 2117. 2267. 2176. 2101. 2073. 2189. 2066. 2333. 2134. 2026.\n",
      "  2183. 2368. 2022. 4150. 1934. 2172. 2330. 3916. 2394. 2451. 2215. 2088.\n",
      "     0. 2256. 2199. 2158. 1936. 2332.]\n",
      " [3751. 2201. 2253. 2155. 2221. 2305. 2361. 2217. 2187. 2330. 2196. 2275.\n",
      "  2127. 2392. 2238. 4014. 2107. 2320. 2575. 3821. 2484. 2404. 2303. 2302.\n",
      "  2256.    0. 2221. 2258. 2123. 2407.]\n",
      " [3702. 2220. 2170. 2138. 2055. 2310. 2272. 2224. 2107. 2369. 2257. 2155.\n",
      "  2136. 2552. 2266. 4023. 2127. 2212. 2639. 3762. 2433. 2411. 2266. 2248.\n",
      "  2199. 2221.    0. 2168. 2069. 2364.]\n",
      " [3633. 2173. 2255. 2224. 2229. 2266. 2301. 2271. 2139. 2459. 2229. 2259.\n",
      "  2126. 2369. 2296. 3969. 2089. 2272. 2651. 3668. 2414. 2144. 2152. 2183.\n",
      "  2158. 2258. 2168.    0. 2077. 2400.]\n",
      " [3692. 1866. 2027. 2075. 2061. 2038. 2077. 2136. 2020. 2080. 2041. 1973.\n",
      "  1927. 2405. 1975. 3969. 1911. 1963. 2279. 3742. 2340. 2265. 2086. 2040.\n",
      "  1936. 2123. 2069. 2077.    0. 2237.]\n",
      " [3665. 2408. 2289. 2415. 2329. 2395. 2427. 2446. 2214. 2560. 2445. 2421.\n",
      "  2363. 2678. 2385. 4012. 2291. 2414. 2635. 3733. 2705. 2689. 2581. 2491.\n",
      "  2332. 2407. 2364. 2400. 2237.    0.]]\n"
     ]
    }
   ],
   "source": [
    "def delta_score(models, x_test, threshold):\n",
    "    num_models = len(models)\n",
    "    diffs_matrix = np.zeros((num_models, num_models))\n",
    "\n",
    "    # پیش‌بینی خروجی‌های هر مدل\n",
    "    predictions = [model.predict(x_test) for model in models]\n",
    "    \n",
    "    # محاسبه‌ی argmax برای هر مدل\n",
    "    argmax_preds = [np.argmax(pred, axis=1) for pred in predictions]\n",
    "\n",
    "    # مقایسه مدل‌ها دو به دو\n",
    "    for i in range(num_models):\n",
    "        for j in range(i + 1, num_models):\n",
    "            diff_count = 0  # شمارنده اختلافات\n",
    "\n",
    "            # مقایسه‌ی argmax ها\n",
    "            argmax_diff = argmax_preds[i] != argmax_preds[j]\n",
    "            diff_count += np.sum(argmax_diff)  # اضافه کردن اختلاف در argmax\n",
    "            \n",
    "            # مقایسه‌ی احتمالات برای پیش‌بینی‌های با argmax یکسان\n",
    "            same_argmax_indices = np.where(argmax_preds[i] == argmax_preds[j])[0]\n",
    "            \n",
    "            for idx in same_argmax_indices:\n",
    "                # مقایسه‌ی احتمالات در همان کلاس‌های argmax\n",
    "                prob_i = round(predictions[i][idx][argmax_preds[i][idx]], 2)\n",
    "                prob_j = round(predictions[j][idx][argmax_preds[j][idx]], 2)\n",
    "                \n",
    "                # محاسبه اختلاف بین پیش‌بینی‌ها\n",
    "                prob_diff = abs(prob_i - prob_j)\n",
    "                \n",
    "                # بررسی اختلاف با آستانه threshold\n",
    "                if prob_diff >= threshold:\n",
    "                    diff_count += 1  # شمارش اختلاف در پیش‌بینی‌ها\n",
    "\n",
    "            # ذخیره اختلافات در هر دو موقعیت (i, j) و (j, i) برای متقارن بودن\n",
    "            diffs_matrix[i, j] = diff_count\n",
    "            diffs_matrix[j, i] = diff_count\n",
    "\n",
    "    return diffs_matrix\n",
    "\n",
    "# اجرای تابع با تعیین آستانه threshold\n",
    "diff_score = delta_score(client_models, x_test, threshold=0.1)\n",
    "print(\"Distance Matrix for Delta Score:\")\n",
    "print(diff_score)\n",
    "\n",
    "# start_time = time.time()\n",
    "# end_time = time.time()\n",
    "# execution_time = end_time - start_time\n",
    "# print(f\"\\nExecution time for delta_score: {execution_time:.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907d4da0-0228-4dd5-a914-a448216d7772",
   "metadata": {},
   "source": [
    "### KS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0dda1adf-1401-47d2-9067-bdf49f862091",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance Matrix for KS:\n",
      "[[0.     0.6837 0.6857 0.6735 0.6865 0.6806 0.6841 0.6813 0.6829 0.6795\n",
      "  0.6865 0.682  0.6831 0.6828 0.6823 0.0416 0.6811 0.6823 0.6847 0.0214\n",
      "  0.6765 0.6837 0.6851 0.683  0.6858 0.6836 0.6867 0.6757 0.6827 0.676 ]\n",
      " [0.6837 0.     0.0356 0.0568 0.0485 0.0364 0.0573 0.0455 0.0825 0.024\n",
      "  0.0691 0.0226 0.0342 0.0574 0.0192 0.6422 0.043  0.0556 0.0789 0.6753\n",
      "  0.0497 0.0486 0.0419 0.0287 0.0291 0.0632 0.0472 0.0562 0.0571 0.0998]\n",
      " [0.6857 0.0356 0.     0.0571 0.0165 0.0143 0.0266 0.0188 0.0919 0.0308\n",
      "  0.0448 0.0188 0.0396 0.0349 0.0221 0.6445 0.0505 0.0244 0.0527 0.6765\n",
      "  0.0296 0.0274 0.0189 0.0268 0.0148 0.0325 0.0573 0.0667 0.07   0.1071]\n",
      " [0.6735 0.0568 0.0571 0.     0.0694 0.0465 0.058  0.0477 0.0595 0.0438\n",
      "  0.0836 0.0451 0.0338 0.0493 0.0551 0.632  0.0268 0.061  0.0772 0.6655\n",
      "  0.0395 0.044  0.0515 0.0353 0.0591 0.0537 0.0515 0.043  0.0459 0.0707]\n",
      " [0.6865 0.0485 0.0165 0.0694 0.     0.0257 0.0203 0.027  0.1045 0.0441\n",
      "  0.0316 0.0308 0.053  0.0294 0.0306 0.6453 0.0649 0.0168 0.0418 0.6773\n",
      "  0.0388 0.0329 0.0325 0.04   0.0287 0.0271 0.0695 0.0795 0.0852 0.1218]\n",
      " [0.6806 0.0364 0.0143 0.0465 0.0257 0.     0.0303 0.0245 0.098  0.0385\n",
      "  0.0391 0.0254 0.0453 0.0376 0.0213 0.6391 0.0564 0.0268 0.0462 0.6726\n",
      "  0.0303 0.0326 0.0232 0.021  0.026  0.0364 0.0606 0.0778 0.0777 0.1093]\n",
      " [0.6841 0.0573 0.0266 0.058  0.0203 0.0303 0.     0.0163 0.0983 0.051\n",
      "  0.0341 0.0386 0.0584 0.0293 0.0457 0.6429 0.0604 0.0109 0.037  0.675\n",
      "  0.0272 0.0243 0.0311 0.0309 0.0328 0.0148 0.0616 0.0844 0.0892 0.1128]\n",
      " [0.6813 0.0455 0.0188 0.0477 0.027  0.0245 0.0163 0.     0.0982 0.0389\n",
      "  0.0398 0.0294 0.048  0.0328 0.0367 0.6398 0.0564 0.0186 0.0434 0.6733\n",
      "  0.0195 0.0204 0.0251 0.0219 0.0271 0.022  0.0616 0.0782 0.08   0.1088]\n",
      " [0.6829 0.0825 0.0919 0.0595 0.1045 0.098  0.0983 0.0982 0.     0.063\n",
      "  0.1315 0.075  0.0586 0.0829 0.0895 0.6414 0.0439 0.0931 0.132  0.6749\n",
      "  0.0857 0.0833 0.0762 0.0798 0.0794 0.0999 0.0429 0.0312 0.0287 0.0328]\n",
      " [0.6795 0.024  0.0308 0.0438 0.0441 0.0385 0.051  0.0389 0.063  0.\n",
      "  0.0693 0.0186 0.0253 0.0462 0.0291 0.638  0.0233 0.0494 0.0758 0.6715\n",
      "  0.0398 0.0333 0.0265 0.0229 0.0243 0.0586 0.0288 0.0438 0.0466 0.0786]\n",
      " [0.6865 0.0691 0.0448 0.0836 0.0316 0.0391 0.0341 0.0398 0.1315 0.0693\n",
      "  0.     0.058  0.0765 0.0514 0.0521 0.6452 0.0896 0.041  0.0191 0.6772\n",
      "  0.0501 0.05   0.0567 0.0573 0.0571 0.0416 0.0937 0.1084 0.1089 0.1457]\n",
      " [0.682  0.0226 0.0188 0.0451 0.0308 0.0254 0.0386 0.0294 0.075  0.0186\n",
      "  0.058  0.     0.0252 0.0401 0.0172 0.6405 0.0361 0.0365 0.0673 0.674\n",
      "  0.0328 0.0348 0.0269 0.0136 0.015  0.0461 0.0398 0.0537 0.0571 0.0921]\n",
      " [0.6831 0.0342 0.0396 0.0338 0.053  0.0453 0.0584 0.048  0.0586 0.0253\n",
      "  0.0765 0.0252 0.     0.0575 0.0392 0.6416 0.017  0.0566 0.0848 0.6749\n",
      "  0.048  0.0426 0.0382 0.0309 0.0304 0.0652 0.0205 0.0349 0.0396 0.0724]\n",
      " [0.6828 0.0574 0.0349 0.0493 0.0294 0.0376 0.0293 0.0328 0.0829 0.0462\n",
      "  0.0514 0.0401 0.0575 0.     0.0466 0.6413 0.0603 0.0302 0.051  0.6748\n",
      "  0.0205 0.0201 0.0275 0.0336 0.0375 0.025  0.0495 0.0761 0.0812 0.0965]\n",
      " [0.6823 0.0192 0.0221 0.0551 0.0306 0.0213 0.0457 0.0367 0.0895 0.0291\n",
      "  0.0521 0.0172 0.0392 0.0466 0.     0.6408 0.0506 0.0409 0.0643 0.6743\n",
      "  0.0376 0.0382 0.0309 0.0269 0.0163 0.0514 0.0548 0.06   0.0623 0.1052]\n",
      " [0.0416 0.6422 0.6445 0.632  0.6453 0.6391 0.6429 0.6398 0.6414 0.638\n",
      "  0.6452 0.6405 0.6416 0.6413 0.6408 0.     0.6396 0.6408 0.6435 0.0335\n",
      "  0.635  0.6422 0.6436 0.6415 0.6443 0.6424 0.6455 0.6342 0.6412 0.6345]\n",
      " [0.6811 0.043  0.0505 0.0268 0.0649 0.0564 0.0604 0.0564 0.0439 0.0233\n",
      "  0.0896 0.0361 0.017  0.0603 0.0506 0.6396 0.     0.0591 0.0909 0.6731\n",
      "  0.0507 0.0487 0.0409 0.0376 0.0407 0.0702 0.0274 0.0286 0.0323 0.0576]\n",
      " [0.6823 0.0556 0.0244 0.061  0.0168 0.0268 0.0109 0.0186 0.0931 0.0494\n",
      "  0.041  0.0365 0.0566 0.0302 0.0409 0.6408 0.0591 0.     0.0442 0.6743\n",
      "  0.03   0.0225 0.0252 0.0286 0.0311 0.018  0.0571 0.079  0.0854 0.107 ]\n",
      " [0.6847 0.0789 0.0527 0.0772 0.0418 0.0462 0.037  0.0434 0.132  0.0758\n",
      "  0.0191 0.0673 0.0848 0.051  0.0643 0.6435 0.0909 0.0442 0.     0.6756\n",
      "  0.0527 0.0535 0.0631 0.0634 0.0647 0.036  0.0969 0.1187 0.121  0.1417]\n",
      " [0.0214 0.6753 0.6765 0.6655 0.6773 0.6726 0.675  0.6733 0.6749 0.6715\n",
      "  0.6772 0.674  0.6749 0.6748 0.6743 0.0335 0.6731 0.6743 0.6756 0.\n",
      "  0.6685 0.6753 0.6762 0.6747 0.6769 0.6747 0.6774 0.6677 0.6747 0.668 ]\n",
      " [0.6765 0.0497 0.0296 0.0395 0.0388 0.0303 0.0272 0.0195 0.0857 0.0398\n",
      "  0.0501 0.0328 0.048  0.0205 0.0376 0.635  0.0507 0.03   0.0527 0.6685\n",
      "  0.     0.0214 0.0266 0.0258 0.0325 0.0228 0.0478 0.071  0.0768 0.0995]\n",
      " [0.6837 0.0486 0.0274 0.044  0.0329 0.0326 0.0243 0.0204 0.0833 0.0333\n",
      "  0.05   0.0348 0.0426 0.0201 0.0382 0.6422 0.0487 0.0225 0.0535 0.6753\n",
      "  0.0214 0.     0.0189 0.0293 0.0316 0.0327 0.0471 0.0665 0.0694 0.096 ]\n",
      " [0.6851 0.0419 0.0189 0.0515 0.0325 0.0232 0.0311 0.0251 0.0762 0.0265\n",
      "  0.0567 0.0269 0.0382 0.0275 0.0309 0.6436 0.0409 0.0252 0.0631 0.6762\n",
      "  0.0266 0.0189 0.     0.0195 0.0236 0.0363 0.0397 0.0563 0.0638 0.0908]\n",
      " [0.683  0.0287 0.0268 0.0353 0.04   0.021  0.0309 0.0219 0.0798 0.0229\n",
      "  0.0573 0.0136 0.0309 0.0336 0.0269 0.6415 0.0376 0.0286 0.0634 0.6747\n",
      "  0.0258 0.0293 0.0195 0.     0.0264 0.038  0.0418 0.0583 0.0595 0.0909]\n",
      " [0.6858 0.0291 0.0148 0.0591 0.0287 0.026  0.0328 0.0271 0.0794 0.0243\n",
      "  0.0571 0.015  0.0304 0.0375 0.0163 0.6443 0.0407 0.0311 0.0647 0.6769\n",
      "  0.0325 0.0316 0.0236 0.0264 0.     0.0387 0.0443 0.057  0.0619 0.0948]\n",
      " [0.6836 0.0632 0.0325 0.0537 0.0271 0.0364 0.0148 0.022  0.0999 0.0586\n",
      "  0.0416 0.0461 0.0652 0.025  0.0514 0.6424 0.0702 0.018  0.036  0.6747\n",
      "  0.0228 0.0327 0.0363 0.038  0.0387 0.     0.0648 0.0902 0.0965 0.109 ]\n",
      " [0.6867 0.0472 0.0573 0.0515 0.0695 0.0606 0.0616 0.0616 0.0429 0.0288\n",
      "  0.0937 0.0398 0.0205 0.0495 0.0548 0.6455 0.0274 0.0571 0.0969 0.6774\n",
      "  0.0478 0.0471 0.0397 0.0418 0.0443 0.0648 0.     0.0356 0.0482 0.0616]\n",
      " [0.6757 0.0562 0.0667 0.043  0.0795 0.0778 0.0844 0.0782 0.0312 0.0438\n",
      "  0.1084 0.0537 0.0349 0.0761 0.06   0.6342 0.0286 0.079  0.1187 0.6677\n",
      "  0.071  0.0665 0.0563 0.0583 0.057  0.0902 0.0356 0.     0.0211 0.0486]\n",
      " [0.6827 0.0571 0.07   0.0459 0.0852 0.0777 0.0892 0.08   0.0287 0.0466\n",
      "  0.1089 0.0571 0.0396 0.0812 0.0623 0.6412 0.0323 0.0854 0.121  0.6747\n",
      "  0.0768 0.0694 0.0638 0.0595 0.0619 0.0965 0.0482 0.0211 0.     0.0463]\n",
      " [0.676  0.0998 0.1071 0.0707 0.1218 0.1093 0.1128 0.1088 0.0328 0.0786\n",
      "  0.1457 0.0921 0.0724 0.0965 0.1052 0.6345 0.0576 0.107  0.1417 0.668\n",
      "  0.0995 0.096  0.0908 0.0909 0.0948 0.109  0.0616 0.0486 0.0463 0.    ]]\n"
     ]
    }
   ],
   "source": [
    "def ks(models, x_test):\n",
    "    num_samples = x_test.shape[0]\n",
    "    num_models = len(models)\n",
    "\n",
    "    # مرحله 1: پیش‌بینی کلاس‌ها برای داده‌های تست توسط هر مدل\n",
    "    argmax_preds = np.array([model.predict(x_test).argmax(axis=1) for model in client_models])\n",
    "\n",
    "    # مرحله 2: یافتن کلاس پرتکرار برای هر نمونه\n",
    "    most_frequent_classes = [Counter(argmax_preds[:, i]).most_common(1)[0][0] for i in range(num_samples)]\n",
    "\n",
    "    # مرحله 3: استخراج احتمال کلاس پرتکرار از تمامی مدل‌ها برای هر نمونه\n",
    "    all_probabilities = np.zeros((num_samples, num_clients))\n",
    "    for i, model in enumerate(client_models):\n",
    "        predictions = model.predict(x_test)  # احتمالات کلاس‌های مختلف برای هر نمونه\n",
    "        for j, sample in enumerate(x_test):\n",
    "            most_frequent_class = most_frequent_classes[j]\n",
    "            all_probabilities[j, i] = predictions[j, most_frequent_class]  # احتمال کلاس پرتکرار\n",
    "    # مرحله 4: انجام تست KS بین مدل‌های کلاینت و ایجاد ماتریس KS\n",
    "    ks_distance_matrix = np.zeros((num_clients, num_clients))\n",
    "    for i in range(num_clients):\n",
    "        for j in range(i + 1, num_clients):\n",
    "            # تست KS برای مقایسه احتمالات دو مدل کلاینت\n",
    "            ks_statistic, _ = ks_2samp(all_probabilities[:, i], all_probabilities[:, j])\n",
    "            ks_distance_matrix[i, j] = ks_statistic\n",
    "            ks_distance_matrix[j, i] = ks_statistic  # ماتریس متقارن\n",
    "\n",
    "    return ks_distance_matrix\n",
    "\n",
    "# اجرای تابع با تعیین تعداد ارقام اعشار\n",
    "matrix_ks = ks(client_models, x_test)\n",
    "print(\"Distance Matrix for KS:\")\n",
    "print(matrix_ks)\n",
    "\n",
    "# start_time = time.time()\n",
    "# end_time = time.time()\n",
    "# execution_time = end_time - start_time\n",
    "# print(f\"\\nExecution time for delta_class: {execution_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be14f48-75e7-4874-9c73-7a91c69af338",
   "metadata": {},
   "source": [
    "### Chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "625c0aa8-a02a-4887-80fc-1a2d6718ff07",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance Matrix for Chi square:\n",
      "[[    0.         44204.05264391 45219.64430531 46704.60303096\n",
      "  45165.22662329 45758.02517846 45346.93394299 45732.71581961\n",
      "  44652.72888619 44189.32797093 44606.01625262 44963.68993944\n",
      "  44856.1157383  43381.58023074 45227.04395038 52485.17251841\n",
      "  46014.08751545 46073.99255923 43506.97094915 61638.7852426\n",
      "  45245.70402613 44999.76562982 44858.00216026 45194.63172083\n",
      "  44755.93310349 46013.81524538 45200.96727567 45461.43901741\n",
      "  45244.36771994 44747.52845805]\n",
      " [44204.05264391     0.         74579.26941519 74248.82223184\n",
      "  74549.40471335 74038.78813315 74905.71110643 74252.05515549\n",
      "  74512.02711113 73274.66281022 74913.51974548 75494.64662477\n",
      "  75921.49507832 71673.82163319 75109.75332241 40716.97985303\n",
      "  75089.72913101 73775.15821024 72027.28229034 43669.85835685\n",
      "  72899.34957354 72351.74899499 74462.63335868 74151.97601477\n",
      "  75055.12230505 74533.82028145 73965.70549347 73439.58981652\n",
      "  76158.03674885 70680.30694156]\n",
      " [45219.64430531 74579.26941519     0.         75774.51949975\n",
      "  75494.69594851 74812.96048071 75998.67435909 74874.04008647\n",
      "  75602.4594113  74681.43804204 73610.67722929 75486.15204858\n",
      "  75897.16565727 71784.13733129 75151.4419453  41126.08858475\n",
      "  76736.35907422 75354.41084429 74057.53753569 44784.70373212\n",
      "  72993.60419841 73194.58853409 74018.95769864 74698.21932927\n",
      "  76306.47869132 74907.59067323 75405.24779473 74032.13976713\n",
      "  75707.15988488 72773.9673555 ]\n",
      " [46704.60303096 74248.82223184 75774.51949975     0.\n",
      "  74670.31446212 74273.70117016 74158.65474206 74894.24852411\n",
      "  74768.723351   74450.31758076 73575.67757565 74479.45068268\n",
      "  76175.01798345 71471.15951508 74340.82927991 42276.71668451\n",
      "  75693.1995704  75321.58182541 71759.84908345 46187.6347778\n",
      "  73122.57972723 73390.59651467 74381.29257308 74752.62051442\n",
      "  74711.88982334 75583.5991794  75230.48350061 74194.21892098\n",
      "  75740.88507157 71722.65645448]\n",
      " [45165.22662329 74549.40471335 75494.69594851 74670.31446212\n",
      "      0.         74181.44212278 74597.295203   74244.58161633\n",
      "  75392.46377897 72882.97728447 73426.45780635 74290.05572545\n",
      "  76818.67087043 70543.58160502 74275.16559653 41479.62055574\n",
      "  75617.37443065 73970.87217659 71381.12997444 44816.18263961\n",
      "  71528.2274229  72529.8139131  74203.80767145 74413.73167896\n",
      "  75152.00779272 74096.29828595 75506.12147106 73458.50538779\n",
      "  75321.30954198 71880.89540129]\n",
      " [45758.02517846 74038.78813315 74812.96048071 74273.70117016\n",
      "  74181.44212278     0.         74934.62235366 74058.19252573\n",
      "  73497.31383145 72397.06288129 73193.70396665 74309.32384145\n",
      "  74223.37547991 72321.91603554 75621.9498009  41232.81499186\n",
      "  73581.11795973 74496.96376943 71712.61337515 45152.36042578\n",
      "  73584.38311296 73036.83201348 73222.43444267 74312.30741407\n",
      "  75168.47262732 73472.16815221 72979.02312139 73780.53488679\n",
      "  75232.92621385 70966.24055011]\n",
      " [45346.93394299 74905.71110643 75998.67435909 74158.65474206\n",
      "  74597.295203   74934.62235366     0.         74802.44154494\n",
      "  75199.41214212 73666.61810719 75008.69401457 75731.01605052\n",
      "  74438.32343713 70917.69581817 75906.50818175 41728.52770433\n",
      "  76132.03876637 74562.78633866 73822.04430408 44749.23109596\n",
      "  73066.14376286 72886.04743254 74264.61482478 75726.70777348\n",
      "  76192.71191674 74186.3026615  75360.44670297 73581.49523319\n",
      "  75251.32543914 71575.1079739 ]\n",
      " [45732.71581961 74252.05515549 74874.04008647 74894.24852411\n",
      "  74244.58161633 74058.19252573 74802.44154494     0.\n",
      "  74956.9212299  72577.10896401 74982.39406323 74453.711362\n",
      "  74823.8162004  71845.47123848 75203.07414132 41508.65823646\n",
      "  75794.53995629 75074.70828472 72484.29944495 45043.90166335\n",
      "  72341.29659324 72697.67608085 75664.64480097 75629.58890948\n",
      "  75827.79512668 74734.4414286  74603.04176713 74408.46649981\n",
      "  75929.44561799 71631.10528222]\n",
      " [44652.72888619 74512.02711113 75602.4594113  74768.723351\n",
      "  75392.46377897 73497.31383145 75199.41214212 74956.9212299\n",
      "      0.         72362.90602764 74297.15302231 74289.28233695\n",
      "  75206.67517524 71169.84593535 75173.85315775 40351.88783802\n",
      "  76159.94656337 74265.28436824 73042.32748499 44103.8363021\n",
      "  71608.97831463 72086.2920289  73746.16806151 74869.48707767\n",
      "  75658.83010396 74258.15069558 75132.77154264 74441.91345558\n",
      "  75553.43505712 72286.34936091]\n",
      " [44189.32797093 73274.66281022 74681.43804204 74450.31758076\n",
      "  72882.97728447 72397.06288129 73666.61810719 72577.10896401\n",
      "  72362.90602764     0.         71605.89936074 73781.05220007\n",
      "  73854.79075914 69419.2014089  74324.61956775 40688.89983054\n",
      "  73404.20469065 74109.51483017 72845.05209981 44100.35677866\n",
      "  70556.67790482 70983.31399304 72676.18068999 72345.85005168\n",
      "  73654.17573316 73013.07658421 72676.65480036 71271.95253444\n",
      "  74770.69783527 70068.55383361]\n",
      " [44606.01625262 74913.51974548 73610.67722929 73575.67757565\n",
      "  73426.45780635 73193.70396665 75008.69401457 74982.39406323\n",
      "  74297.15302231 71605.89936074     0.         73970.850308\n",
      "  74342.20971713 71770.39373311 73409.1990772  41116.61446185\n",
      "  74469.64081268 72186.01136932 72424.50007017 44502.23093327\n",
      "  70902.7059463  72218.56610364 73764.61433993 74988.40469065\n",
      "  75148.63990749 74166.72612543 74072.9393334  73656.97966572\n",
      "  75084.68281693 70906.88415847]\n",
      " [44963.68993944 75494.64662477 75486.15204858 74479.45068268\n",
      "  74290.05572545 74309.32384145 75731.01605052 74453.711362\n",
      "  74289.28233695 73781.05220007 73970.850308       0.\n",
      "  74362.90946555 70616.44865643 76514.70900341 41323.2906913\n",
      "  74952.53304342 74890.69618198 73795.67569051 44454.04669502\n",
      "  73084.20565366 72481.19564925 74078.52341785 75594.41112751\n",
      "  76056.95457451 73132.34021809 74173.07806101 72785.68767136\n",
      "  75526.30870488 70836.6178894 ]\n",
      " [44856.1157383  75921.49507832 75897.16565727 76175.01798345\n",
      "  76818.67087043 74223.37547991 74438.32343713 74823.8162004\n",
      "  75206.67517524 73854.79075914 74342.20971713 74362.90946555\n",
      "      0.         71973.81629617 74657.50654736 41288.6316976\n",
      "  75837.60329143 75183.2890209  72508.19698396 44652.80827861\n",
      "  71507.64173088 73836.42798833 75599.8774182  74565.75213716\n",
      "  75031.48275568 74990.12771589 75518.6971111  74594.9559188\n",
      "  77006.53652253 71039.47066504]\n",
      " [43381.58023074 71673.82163319 71784.13733129 71471.15951508\n",
      "  70543.58160502 72321.91603554 70917.69581817 71845.47123848\n",
      "  71169.84593535 69419.2014089  71770.39373311 70616.44865643\n",
      "  71973.81629617     0.         72075.75548971 39725.0432864\n",
      "  72293.65945082 70714.33987922 68995.34373655 42875.61459197\n",
      "  69459.80551849 70608.12990358 70466.33763704 71545.62048683\n",
      "  72580.75769984 72652.02347639 70249.90892545 72287.41369184\n",
      "  71615.7720608  68539.57516357]\n",
      " [45227.04395038 75109.75332241 75151.4419453  74340.82927991\n",
      "  74275.16559653 75621.9498009  75906.50818175 75203.07414132\n",
      "  75173.85315775 74324.61956775 73409.1990772  76514.70900341\n",
      "  74657.50654736 72075.75548971     0.         41500.47522349\n",
      "  75018.01728556 76023.27711943 74146.68931213 44368.75502041\n",
      "  73649.85576117 72809.18505025 74505.12312957 75521.36759814\n",
      "  76391.25522639 74416.54363169 74006.71078217 73812.49045781\n",
      "  75816.34147091 71262.96397539]\n",
      " [52485.17251841 40716.97985303 41126.08858475 42276.71668451\n",
      "  41479.62055574 41232.81499186 41728.52770433 41508.65823646\n",
      "  40351.88783802 40688.89983054 41116.61446185 41323.2906913\n",
      "  41288.6316976  39725.0432864  41500.47522349     0.\n",
      "  41613.59184041 41652.90444715 39407.90223042 54098.52645381\n",
      "  41298.04170436 41993.0356299  41246.98657768 41401.11408467\n",
      "  40993.86197046 42269.17302819 41036.86074041 41265.01427122\n",
      "  41578.53842255 40966.72231026]\n",
      " [46014.08751545 75089.72913101 76736.35907422 75693.1995704\n",
      "  75617.37443065 73581.11795973 76132.03876637 75794.53995629\n",
      "  76159.94656337 73404.20469065 74469.64081268 74952.53304342\n",
      "  75837.60329143 72293.65945082 75018.01728556 41613.59184041\n",
      "      0.         75393.805242   74135.59773044 45135.7778047\n",
      "  72799.66873061 73322.50929404 74505.9677775  75088.38287638\n",
      "  76992.28709167 75008.59143877 75120.11952529 74871.00891765\n",
      "  75827.91589799 71329.8334284 ]\n",
      " [46073.99255923 73775.15821024 75354.41084429 75321.58182541\n",
      "  73970.87217659 74496.96376943 74562.78633866 75074.70828472\n",
      "  74265.28436824 74109.51483017 72186.01136932 74890.69618198\n",
      "  75183.2890209  70714.33987922 76023.27711943 41652.90444715\n",
      "  75393.805242       0.         72706.16953869 45338.17203576\n",
      "  73801.02182025 72962.99203956 75435.03218751 75156.21454074\n",
      "  75302.66212248 73894.2134625  74036.94925615 73121.45618703\n",
      "  76258.53452146 70704.02542761]\n",
      " [43506.97094915 72027.28229034 74057.53753569 71759.84908345\n",
      "  71381.12997444 71712.61337515 73822.04430408 72484.29944495\n",
      "  73042.32748499 72845.05209981 72424.50007017 73795.67569051\n",
      "  72508.19698396 68995.34373655 74146.68931213 39407.90223042\n",
      "  74135.59773044 72706.16953869     0.         43357.48848241\n",
      "  70138.9055599  69146.21232119 71675.70935738 72515.07638247\n",
      "  74087.98738272 72078.97449634 71283.53380108 70722.51800061\n",
      "  73610.16372973 68750.38487858]\n",
      " [61638.7852426  43669.85835685 44784.70373212 46187.6347778\n",
      "  44816.18263961 45152.36042578 44749.23109596 45043.90166335\n",
      "  44103.8363021  44100.35677866 44502.23093327 44454.04669502\n",
      "  44652.80827861 42875.61459197 44368.75502041 54098.52645381\n",
      "  45135.7778047  45338.17203576 43357.48848241     0.\n",
      "  44331.12733775 44721.18569056 44619.45535505 44355.80816221\n",
      "  44437.95712767 45282.5150237  44645.65139002 44907.6171573\n",
      "  45066.44914719 44400.45158539]\n",
      " [45245.70402613 72899.34957354 72993.60419841 73122.57972723\n",
      "  71528.2274229  73584.38311296 73066.14376286 72341.29659324\n",
      "  71608.97831463 70556.67790482 70902.7059463  73084.20565366\n",
      "  71507.64173088 69459.80551849 73649.85576117 41298.04170436\n",
      "  72799.66873061 73801.02182025 70138.9055599  44331.12733775\n",
      "      0.         71012.1434562  71463.86654422 72521.64461863\n",
      "  73090.3373465  71476.84722635 72237.20312515 72352.40118791\n",
      "  72312.26982315 68545.05296648]\n",
      " [44999.76562982 72351.74899499 73194.58853409 73390.59651467\n",
      "  72529.8139131  73036.83201348 72886.04743254 72697.67608085\n",
      "  72086.2920289  70983.31399304 72218.56610364 72481.19564925\n",
      "  73836.42798833 70608.12990358 72809.18505025 41993.0356299\n",
      "  73322.50929404 72962.99203956 69146.21232119 44721.18569056\n",
      "  71012.1434562      0.         73322.73937834 72060.15851956\n",
      "  73111.67901583 72861.74026011 72708.31338293 74517.34889583\n",
      "  73532.0350975  69244.46715656]\n",
      " [44858.00216026 74462.63335868 74018.95769864 74381.29257308\n",
      "  74203.80767145 73222.43444267 74264.61482478 75664.64480097\n",
      "  73746.16806151 72676.18068999 73764.61433993 74078.52341785\n",
      "  75599.8774182  70466.33763704 74505.12312957 41246.98657768\n",
      "  74505.9677775  75435.03218751 71675.70935738 44619.45535505\n",
      "  71463.86654422 73322.73937834     0.         75230.33278977\n",
      "  75314.68967812 74469.78155893 74590.32775433 74873.96453982\n",
      "  76041.81030817 70688.29133379]\n",
      " [45194.63172083 74151.97601477 74698.21932927 74752.62051442\n",
      "  74413.73167896 74312.30741407 75726.70777348 75629.58890948\n",
      "  74869.48707767 72345.85005168 74988.40469065 75594.41112751\n",
      "  74565.75213716 71545.62048683 75521.36759814 41401.11408467\n",
      "  75088.38287638 75156.21454074 72515.07638247 44355.80816221\n",
      "  72521.64461863 72060.15851956 75230.33278977     0.\n",
      "  76207.67193236 73552.51153896 74118.9041908  74444.55657093\n",
      "  75718.88291169 71488.51898531]\n",
      " [44755.93310349 75055.12230505 76306.47869132 74711.88982334\n",
      "  75152.00779272 75168.47262732 76192.71191674 75827.79512668\n",
      "  75658.83010396 73654.17573316 75148.63990749 76056.95457451\n",
      "  75031.48275568 72580.75769984 76391.25522639 40993.86197046\n",
      "  76992.28709167 75302.66212248 74087.98738272 44437.95712767\n",
      "  73090.3373465  73111.67901583 75314.68967812 76207.67193236\n",
      "      0.         74333.4776725  75087.9391253  74612.09487221\n",
      "  76366.20625847 71459.91635935]\n",
      " [46013.81524538 74533.82028145 74907.59067323 75583.5991794\n",
      "  74096.29828595 73472.16815221 74186.3026615  74734.4414286\n",
      "  74258.15069558 73013.07658421 74166.72612543 73132.34021809\n",
      "  74990.12771589 72652.02347639 74416.54363169 42269.17302819\n",
      "  75008.59143877 73894.2134625  72078.97449634 45282.5150237\n",
      "  71476.84722635 72861.74026011 74469.78155893 73552.51153896\n",
      "  74333.4776725      0.         74122.74721313 73279.85987583\n",
      "  75054.58019841 71074.86354499]\n",
      " [45200.96727567 73965.70549347 75405.24779473 75230.48350061\n",
      "  75506.12147106 72979.02312139 75360.44670297 74603.04176713\n",
      "  75132.77154264 72676.65480036 74072.9393334  74173.07806101\n",
      "  75518.6971111  70249.90892545 74006.71078217 41036.86074041\n",
      "  75120.11952529 74036.94925615 71283.53380108 44645.65139002\n",
      "  72237.20312515 72708.31338293 74590.32775433 74118.9041908\n",
      "  75087.9391253  74122.74721313     0.         74072.25288665\n",
      "  74659.50244823 70860.03499576]\n",
      " [45461.43901741 73439.58981652 74032.13976713 74194.21892098\n",
      "  73458.50538779 73780.53488679 73581.49523319 74408.46649981\n",
      "  74441.91345558 71271.95253444 73656.97966572 72785.68767136\n",
      "  74594.9559188  72287.41369184 73812.49045781 41265.01427122\n",
      "  74871.00891765 73121.45618703 70722.51800061 44907.6171573\n",
      "  72352.40118791 74517.34889583 74873.96453982 74444.55657093\n",
      "  74612.09487221 73279.85987583 74072.25288665     0.\n",
      "  74334.77089668 70637.80134427]\n",
      " [45244.36771994 76158.03674885 75707.15988488 75740.88507157\n",
      "  75321.30954198 75232.92621385 75251.32543914 75929.44561799\n",
      "  75553.43505712 74770.69783527 75084.68281693 75526.30870488\n",
      "  77006.53652253 71615.7720608  75816.34147091 41578.53842255\n",
      "  75827.91589799 76258.53452146 73610.16372973 45066.44914719\n",
      "  72312.26982315 73532.0350975  76041.81030817 75718.88291169\n",
      "  76366.20625847 75054.58019841 74659.50244823 74334.77089668\n",
      "      0.         71687.56998031]\n",
      " [44747.52845805 70680.30694156 72773.9673555  71722.65645448\n",
      "  71880.89540129 70966.24055011 71575.1079739  71631.10528222\n",
      "  72286.34936091 70068.55383361 70906.88415847 70836.6178894\n",
      "  71039.47066504 68539.57516357 71262.96397539 40966.72231026\n",
      "  71329.8334284  70704.02542761 68750.38487858 44400.45158539\n",
      "  68545.05296648 69244.46715656 70688.29133379 71488.51898531\n",
      "  71459.91635935 71074.86354499 70860.03499576 70637.80134427\n",
      "  71687.56998031     0.        ]]\n"
     ]
    }
   ],
   "source": [
    "def chi_square(models, x_test):\n",
    "    num_models = len(models)\n",
    "    num_samples = x_test.shape[0] # تعداد نمونه‌ها  \n",
    "    n_classes = 10  # تعداد کلاس ها\n",
    "\n",
    "    # ساخت ماتریس متقارن برای نتایج Chi-Square\n",
    "    chi2_matrix = np.zeros((num_models, num_models))\n",
    "    all_probabilities = np.zeros((num_samples, num_models))\n",
    "\n",
    "    for i, model in enumerate(models):\n",
    "        predictions = model.predict(x_test)  # احتمالات کلاس‌های مختلف برای هر نمونه\n",
    "        for j, sample in enumerate(x_test):\n",
    "            all_probabilities[j, i] = predictions[j].argmax()\n",
    "\n",
    "    # مرحله: ایجاد جدول‌های متقاطع و انجام تست Chi-Square\n",
    "    for i in range(num_models):\n",
    "        for j in range(i + 1, num_models):\n",
    "            # ساخت جدول متقاطع\n",
    "            contingency_table = pd.crosstab(all_probabilities[:, i], all_probabilities[:, j])\n",
    "            chi2_stat, p_value, _, _ = chi2_contingency(contingency_table)\n",
    "            chi2_matrix[i, j] = chi2_stat\n",
    "            chi2_matrix[j, i] = chi2_stat  # ماتریس متقارن\n",
    "\n",
    "    return chi2_matrix\n",
    "\n",
    "# اجرای تابع با تعیین تعداد ارقام اعشار\n",
    "matrix_chi2 = chi_square(client_models, x_test)\n",
    "print(\"Distance Matrix for Chi square:\")\n",
    "print(matrix_chi2)\n",
    "\n",
    "# start_time = time.time()\n",
    "# end_time = time.time()\n",
    "# execution_time = end_time - start_time\n",
    "# print(f\"\\nExecution time for delta_class: {execution_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377a9d7c-bc15-4ec1-9672-2dc568365e48",
   "metadata": {},
   "source": [
    "### The problematic client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4a15c9-e116-42ae-a2df-8b1353324f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def meancal(matrix):\n",
    "    temp = 0\n",
    "    x = matrix.shape[0]    \n",
    "    arrmean = []\n",
    "    \n",
    "    for i in range(0,x):\n",
    "        #print(matrix[i].mean())\n",
    "        temp = (matrix[i].mean())    \n",
    "        arrmean.append(temp)\n",
    "    return arrmean\n",
    "\n",
    "def iqrfunc(nparray):\n",
    "    data = np.array(nparray)\n",
    "    q1 = np.percentile(data,25)\n",
    "    q3 = np.percentile(data,75)\n",
    "    iqr = q3 -q1\n",
    "    lower_bound = q1-(iqr*1.5)\n",
    "    upper_bound = q3+(iqr*1.5)\n",
    "    outliers = np.where((data<lower_bound) | (data>upper_bound))[0]\n",
    "    return outliers\n",
    "    \n",
    "diff_class = delta_class(client_models, x_test)\n",
    "diff_score = delta_score(client_models, x_test,0.1)\n",
    "diff_ks = ks(client_models, x_test)\n",
    "diff_chi2 = chi_square(client_models, x_test)\n",
    "\n",
    "temp_c = meancal(diff_class)\n",
    "temp_s = meancal(diff_score)\n",
    "temp_ks = meancal(diff_ks)\n",
    "temp_chi2 = meancal(diff_chi2)\n",
    "\n",
    "print(f\"Problemmatic Clients for Delta class {iqrfunc(temp_c)}:\")                       \n",
    "print(f\"Problemmatic Clients for Delta Score {iqrfunc(temp_s)}:\")                                            \n",
    "print(f\"Problemmatic Clients for KS {iqrfunc(temp_ks)}:\")                       \n",
    "print(f\"Problemmatic Clients for X2 {iqrfunc(temp_chi2)}:\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c75fa23-aecb-435e-b35a-17c824cf5b15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "evn8",
   "language": "python",
   "name": "evn8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
